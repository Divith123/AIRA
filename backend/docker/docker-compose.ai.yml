# =============================================================================
# LIVEKIT SELF-HOSTED AI SERVICES
# Complete Docker Compose for local STT, TTS, and LLM
# =============================================================================
#
# Services included:
# - faster-whisper-server: OpenAI-compatible STT (GPU/CPU)
# - kokoro-tts: OpenAI-compatible TTS (GPU/CPU) 
# - piper-tts: Lightweight TTS (CPU only)
# - ollama: Local LLM server
# - redis: Required for LiveKit services coordination
#
# Usage:
#   GPU: docker compose -f docker-compose.ai.yml --profile gpu up -d
#   CPU: docker compose -f docker-compose.ai.yml --profile cpu up -d
#   All: docker compose -f docker-compose.ai.yml up -d
#
# =============================================================================

services:
  # ===========================================================================
  # REDIS - Required for LiveKit services coordination
  # ===========================================================================
  redis:
    image: redis:7-alpine
    container_name: livekit-redis
    restart: unless-stopped
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - livekit-ai
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ===========================================================================
  # STT: faster-whisper-server (GPU)
  # OpenAI-compatible API for speech-to-text
  # Latency: 150-250ms
  # ===========================================================================
  faster-whisper-gpu:
    image: fedirz/faster-whisper-server:latest-cuda
    container_name: whisper-stt-gpu
    profiles: ["gpu", "stt-gpu"]
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      # Model configuration
      - WHISPER__MODEL=${WHISPER_MODEL:-Systran/faster-whisper-large-v3-turbo}
      - WHISPER__DEVICE=cuda
      - WHISPER__COMPUTE_TYPE=${WHISPER_COMPUTE_TYPE:-float16}
      # API configuration
      - WHISPER__HOST=0.0.0.0
      - WHISPER__PORT=8000
    volumes:
      - whisper-models:/root/.cache/huggingface
    networks:
      - livekit-ai
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================================================
  # STT: faster-whisper-server (CPU)
  # For systems without GPU
  # Latency: 300-500ms
  # ===========================================================================
  faster-whisper-cpu:
    image: fedirz/faster-whisper-server:latest-cpu
    container_name: whisper-stt-cpu
    profiles: ["cpu", "stt-cpu"]
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      # Use smaller model for CPU
      - WHISPER__MODEL=${WHISPER_MODEL:-Systran/faster-whisper-small}
      - WHISPER__DEVICE=cpu
      - WHISPER__COMPUTE_TYPE=int8
      - WHISPER__HOST=0.0.0.0
      - WHISPER__PORT=8000
    volumes:
      - whisper-models:/root/.cache/huggingface
    networks:
      - livekit-ai
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G

  # ===========================================================================
  # TTS: Kokoro-FastAPI (GPU)
  # OpenAI-compatible API for text-to-speech
  # Latency: 51-84ms (Best open-source option!)
  # ===========================================================================
  kokoro-tts-gpu:
    image: ghcr.io/remsky/kokoro-fastapi-gpu:latest
    container_name: kokoro-tts-gpu
    profiles: ["gpu", "tts-gpu"]
    restart: unless-stopped
    ports:
      - "8880:8880"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - kokoro-voices:/app/voices
    networks:
      - livekit-ai
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================================================
  # TTS: Kokoro-FastAPI (CPU)
  # Latency: ~1s on modern CPUs
  # ===========================================================================
  kokoro-tts-cpu:
    image: ghcr.io/remsky/kokoro-fastapi-cpu:latest
    container_name: kokoro-tts-cpu
    profiles: ["cpu", "tts-cpu"]
    restart: unless-stopped
    ports:
      - "8880:8880"
    volumes:
      - kokoro-voices:/app/voices
    networks:
      - livekit-ai
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G

  # ===========================================================================
  # TTS: Piper TTS (CPU Only)
  # Lightweight alternative for edge/embedded
  # Latency: 50-150ms
  # ===========================================================================
  piper-tts:
    image: rhasspy/wyoming-piper:latest
    container_name: piper-tts
    profiles: ["cpu", "tts-piper"]
    restart: unless-stopped
    ports:
      - "10200:10200"
    volumes:
      - piper-voices:/data
    command: --voice ${PIPER_VOICE:-en_US-lessac-medium}
    networks:
      - livekit-ai

  # ===========================================================================
  # LLM: Ollama
  # Local LLM server supporting llama, mistral, gemma, etc.
  # TTFT: 150-300ms (GPU), 300-500ms (CPU)
  # ===========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-llm
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_MODELS=/models
    volumes:
      - ollama-models:/root/.ollama
    networks:
      - livekit-ai
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================================================
  # Ollama Model Initializer
  # Automatically pulls the configured model on first start
  # ===========================================================================
  ollama-init:
    image: curlimages/curl:latest
    container_name: ollama-init
    profiles: ["init"]
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - livekit-ai
    entrypoint: >
      sh -c "
        echo 'Pulling LLM model: ${LOCAL_LLM_MODEL:-llama3.2:3b}...'
        curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"${LOCAL_LLM_MODEL:-llama3.2:3b}\"}'
        echo 'Model ready!'
      "

# =============================================================================
# VOLUMES
# =============================================================================
volumes:
  redis-data:
    driver: local
  whisper-models:
    driver: local
  kokoro-voices:
    driver: local
  piper-voices:
    driver: local
  ollama-models:
    driver: local

# =============================================================================
# NETWORKS
# =============================================================================
networks:
  livekit-ai:
    driver: bridge
